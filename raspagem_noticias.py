# utils.py

import gspread
from oauth2client.service_account import ServiceAccountCredentials
from datetime import datetime
import pytz
import urllib3

# Evita avisos SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def initialize_sheet():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)
    client = gspread.authorize(creds)
    return client.open_by_key('1G81BndSPpnViMDxRKQCth8PwK0xmAwH-w-T7FjgnwcY')

def get_or_create_worksheet(sheet, name, headers=None):
    try:
        return sheet.worksheet(name)
    except gspread.exceptions.WorksheetNotFound:
        ws = sheet.add_worksheet(title=name, rows="100", cols="20")
        if headers:
            ws.append_row(headers)
        return ws

def get_already_scraped_urls(sheet):
    urls_sheet = get_or_create_worksheet(sheet, "URLs", headers=["URLs"])
    return set(urls_sheet.col_values(1)[1:])

def add_scraped_url(sheet, url):
    urls_sheet = get_or_create_worksheet(sheet, "URLs", headers=["URLs"])
    urls_sheet.append_row([url])

def hoje_brasil_str():
    return datetime.now(pytz.timezone('America/Sao_Paulo')).strftime("%d/%m/%Y")

def hoje_brasil_dt():
    return datetime.now(pytz.timezone('America/Sao_Paulo'))

# ministerios.py

import requests
from bs4 import BeautifulSoup
from datetime import datetime
from utils import (
    initialize_sheet,
    get_already_scraped_urls,
    add_scraped_url,
    get_or_create_worksheet,
    hoje_brasil_str,
    hoje_brasil_dt,
)

HEADERS = {'User-Agent': 'Mozilla/5.0'}

def processar_esporte(sheet, url, data_desejada=None):
    data_desejada = data_desejada or hoje_brasil_str()
    gov_sheet = get_or_create_worksheet(sheet, "gov", headers=["Data", "Minist√©rio", "Subt√≠tulo", "T√≠tulo", "Descri√ß√£o", "URL"])
    urls_ja_lidas = get_already_scraped_urls(sheet)

    try:
        html = requests.get(url, headers=HEADERS, verify=False).text
        soup = BeautifulSoup(html, 'html.parser')
        nome_ministerio = soup.find('meta', property="og:site_name")
        nome_ministerio = nome_ministerio['content'] if nome_ministerio else "Minist√©rio do Esporte"

        novas_linhas = []

        for noticia in soup.find_all('li'):
            data_tag = noticia.find('span', class_='data')
            if not data_tag:
                continue
            try:
                data_dt = datetime.strptime(data_tag.text.strip(), "%d/%m/%Y")
            except ValueError:
                continue
            if data_dt.strftime("%d/%m/%Y") != data_desejada:
                continue

            titulo_tag = noticia.find('h2', class_='titulo')
            link_tag = titulo_tag.find('a') if titulo_tag else None
            if not link_tag or 'href' not in link_tag.attrs:
                continue

            url_noticia = link_tag['href']
            if url_noticia in urls_ja_lidas:
                continue

            subtitulo_tag = noticia.find('div', class_='subtitulo-noticia')
            subtitulo = subtitulo_tag.text.strip() if subtitulo_tag else "Subt√≠tulo n√£o dispon√≠vel"

            descricao_tag = noticia.find('span', class_='descricao')
            descricao = (
                descricao_tag.text.split('-')[1].strip()
                if descricao_tag and '-' in descricao_tag.text
                else (descricao_tag.text.strip() if descricao_tag else "Descri√ß√£o n√£o dispon√≠vel")
            )

            linha = [data_dt, nome_ministerio, subtitulo, titulo_tag.text.strip(), descricao, url_noticia]
            novas_linhas.append(linha)
            add_scraped_url(sheet, url_noticia)

        if novas_linhas:
            gov_sheet.append_rows(novas_linhas, value_input_option='USER_ENTERED')
            print(f"‚úÖ {len(novas_linhas)} not√≠cia(s) adicionada(s) do Esporte.")
        else:
            print("‚ÑπÔ∏è Nenhuma not√≠cia nova do Esporte.")

    except Exception as e:
        print(f"‚ùå Erro Esporte: {e}")

# Adicione aqui as fun√ß√µes: processar_mec, processar_saude, etc.

def processar_mec(sheet, url, data_desejada=None):
    data_desejada = data_desejada or hoje_brasil_str()
    gov_sheet = get_or_create_worksheet(sheet, "gov", headers=["Data", "Minist√©rio", "Subt√≠tulo", "T√≠tulo", "Descri√ß√£o", "URL"])
    urls_ja_lidas = get_already_scraped_urls(sheet)

    try:
        html = requests.get(url, headers=HEADERS, verify=False).text
        soup = BeautifulSoup(html, 'html.parser')
        nome_ministerio = soup.find('meta', property="og:site_name")
        nome_ministerio = nome_ministerio['content'] if nome_ministerio else "Minist√©rio da Educa√ß√£o"

        novas_linhas = []

        for noticia in soup.find_all('li'):
            data_tag = noticia.find('span', class_='data')
            if not data_tag:
                continue
            try:
                data_dt = datetime.strptime(data_tag.text.strip(), "%d/%m/%Y")
            except ValueError:
                continue
            if data_dt.strftime("%d/%m/%Y") != data_desejada:
                continue

            titulo_tag = noticia.find('h2', class_='titulo')
            link_tag = titulo_tag.find('a') if titulo_tag else None
            if not link_tag or 'href' not in link_tag.attrs:
                continue

            url_noticia = link_tag['href']
            if url_noticia in urls_ja_lidas:
                continue

            subtitulo_tag = noticia.find('div', class_='subtitulo-noticia')
            subtitulo = subtitulo_tag.text.strip() if subtitulo_tag else "Subt√≠tulo n√£o dispon√≠vel"

            descricao_tag = noticia.find('span', class_='descricao')
            descricao = (
                descricao_tag.text.split('-')[1].strip()
                if descricao_tag and '-' in descricao_tag.text
                else (descricao_tag.text.strip() if descricao_tag else "Descri√ß√£o n√£o dispon√≠vel")
            )

            linha = [data_dt, nome_ministerio, subtitulo, titulo_tag.text.strip(), descricao, url_noticia]
            novas_linhas.append(linha)
            add_scraped_url(sheet, url_noticia)

        if novas_linhas:
            gov_sheet.append_rows(novas_linhas, value_input_option='USER_ENTERED')
            print(f"‚úÖ {len(novas_linhas)} not√≠cia(s) adicionada(s) do MEC.")
        else:
            print("‚ÑπÔ∏è Nenhuma not√≠cia nova do MEC.")

    except Exception as e:
        print(f"‚ùå Erro MEC: {e}")

def processar_saude(sheet, url, data_desejada=None):
    data_desejada = data_desejada or hoje_brasil_str()
    gov_sheet = get_or_create_worksheet(sheet, "gov", headers=["Data", "Minist√©rio", "Subt√≠tulo", "T√≠tulo", "Descri√ß√£o", "URL"])
    urls_ja_lidas = get_already_scraped_urls(sheet)

    try:
        html = requests.get(url, headers=HEADERS, verify=False).text
        soup = BeautifulSoup(html, 'html.parser')

        nome_tag = soup.find('a', href="https://www.gov.br/saude/pt-br")
        nome_ministerio = nome_tag.text.strip() if nome_tag else "Minist√©rio da Sa√∫de"

        novas_linhas = []

        for noticia in soup.find_all('article', class_='tileItem'):
            data_icon = noticia.find('i', class_='icon-day')
            data_raw = data_icon.find_next_sibling(string=True).strip() if data_icon else None

            try:
                data_dt = datetime.strptime(data_raw, "%d/%m/%Y")
            except Exception:
                continue

            if data_dt.strftime("%d/%m/%Y") != data_desejada:
                continue

            titulo_tag = noticia.find('h2', class_='tileHeadline').find('a')
            titulo = titulo_tag.text.strip() if titulo_tag else "T√≠tulo n√£o dispon√≠vel"
            url_noticia = titulo_tag['href'] if titulo_tag else ""

            if url_noticia in urls_ja_lidas:
                continue

            subtitulo_tag = noticia.find('span', class_='subtitle')
            subtitulo = subtitulo_tag.text.strip() if subtitulo_tag else "Subt√≠tulo n√£o dispon√≠vel"

            descricao_tag = noticia.find('span', class_='description')
            descricao = descricao_tag.text.strip() if descricao_tag else "Descri√ß√£o n√£o dispon√≠vel"

            linha = [data_dt, nome_ministerio, subtitulo, titulo, descricao, url_noticia]
            novas_linhas.append(linha)
            add_scraped_url(sheet, url_noticia)

        if novas_linhas:
            gov_sheet.append_rows(novas_linhas, value_input_option='USER_ENTERED')
            print(f"‚úÖ {len(novas_linhas)} not√≠cia(s) adicionada(s) da Sa√∫de.")
        else:
            print("‚ÑπÔ∏è Nenhuma not√≠cia nova da Sa√∫de.")

    except Exception as e:
        print(f"‚ùå Erro Sa√∫de: {e}")

def processar_povos_indigenas(sheet, url, data_desejada=None):
    data_desejada = data_desejada or hoje_brasil_str()
    gov_sheet = get_or_create_worksheet(sheet, "gov", headers=["Data", "Minist√©rio", "Subt√≠tulo", "T√≠tulo", "Descri√ß√£o", "URL"])
    urls_ja_lidas = get_already_scraped_urls(sheet)

    try:
        html = requests.get(url, headers=HEADERS, verify=False).text
        soup = BeautifulSoup(html, 'html.parser')

        nome_tag = soup.find('a', href="https://www.gov.br/povosindigenas/pt-br")
        nome_ministerio = nome_tag.text.strip() if nome_tag else "Minist√©rio dos Povos Ind√≠genas"

        for noticia in soup.find_all('article', class_='entry'):
            titulo_tag = noticia.find('span', class_='summary').find('a')
            titulo = titulo_tag.text.strip() if titulo_tag else "T√≠tulo n√£o dispon√≠vel"
            url_noticia = titulo_tag['href'] if titulo_tag else ""

            if url_noticia in urls_ja_lidas:
                continue

            data_tag = noticia.find('span', class_='documentByLine')
            if not data_tag:
                continue

            texto_data = data_tag.text.strip()
            if "√∫ltima modifica√ß√£o" not in texto_data:
                continue

            partes = texto_data.split("√∫ltima modifica√ß√£o")
            data_raw = partes[-1].strip().split()[0]

            try:
                data_dt = datetime.strptime(data_raw, "%d/%m/%Y")
            except ValueError:
                continue

            if data_dt.strftime("%d/%m/%Y") != data_desejada:
                continue

            descricao_tag = noticia.find('p', class_='description discreet')
            descricao = descricao_tag.text.strip() if descricao_tag else "Descri√ß√£o n√£o dispon√≠vel"

            linha = [data_dt, nome_ministerio, "N√£o dispon√≠vel", titulo, descricao, url_noticia]
            gov_sheet.append_row(linha)
            add_scraped_url(sheet, url_noticia)

        print(f"‚úÖ Not√≠cias de Povos Ind√≠genas processadas.")

    except Exception as e:
        print(f"‚ùå Erro Povos Ind√≠genas: {e}")

def processar_igualdade(sheet, url, data_desejada=None):
    data_desejada = data_desejada or hoje_brasil_str()
    gov_sheet = get_or_create_worksheet(sheet, "gov", headers=["Data", "Minist√©rio", "Subt√≠tulo", "T√≠tulo", "Descri√ß√£o", "URL"])
    urls_ja_lidas = get_already_scraped_urls(sheet)

    try:
        html = requests.get(url, headers=HEADERS, verify=False).text
        soup = BeautifulSoup(html, 'html.parser')

        nome_tag = soup.find('a', href="https://www.gov.br/igualdaderacial/pt-br")
        nome_ministerio = nome_tag.text.strip() if nome_tag else "Minist√©rio da Igualdade Racial"

        for noticia in soup.find_all('div', class_='conteudo'):
            categoria_tag = noticia.find('div', class_='categoria-noticia')
            subtitulo = categoria_tag.text.strip() if categoria_tag else "Categoria n√£o dispon√≠vel"

            titulo_tag = noticia.find('h2', class_='titulo').find('a')
            titulo = titulo_tag.text.strip() if titulo_tag else "T√≠tulo n√£o dispon√≠vel"
            url_noticia = titulo_tag['href'] if titulo_tag else ""

            if url_noticia in urls_ja_lidas:
                continue

            data_tag = noticia.find('span', class_='data')
            try:
                data_dt = datetime.strptime(data_tag.text.strip(), "%d/%m/%Y")
            except Exception:
                continue

            if data_dt.strftime("%d/%m/%Y") != data_desejada:
                continue

            descricao_tag = noticia.find('span', class_='descricao')
            descricao = descricao_tag.text.strip() if descricao_tag else "Descri√ß√£o n√£o dispon√≠vel"

            linha = [data_dt, nome_ministerio, subtitulo, titulo, descricao, url_noticia]
            gov_sheet.append_row(linha)
            add_scraped_url(sheet, url_noticia)

        print(f"‚úÖ Not√≠cias de Igualdade Racial processadas.")

    except Exception as e:
        print(f"‚ùå Erro Igualdade Racial: {e}")

# main.py

from utils import initialize_sheet
from ministerios import (
    processar_esporte,
    processar_mec,
    processar_saude,
    processar_povos_indigenas,
    processar_igualdade,
)

if __name__ == "__main__":
    sheet = initialize_sheet()

    # Voc√™ pode adicionar/remover minist√©rios conforme desejar
    processar_esporte(sheet, "https://www.gov.br/esporte/pt-br/noticias-e-conteudos/esporte")
    processar_mec(sheet, "https://www.gov.br/mec/pt-br/assuntos/noticias")
    processar_saude(sheet, "https://www.gov.br/saude/pt-br/assuntos/noticias")
    processar_povos_indigenas(sheet, "https://www.gov.br/povosindigenas/pt-br/assuntos/noticias/2025/06-1")
    processar_igualdade(sheet, "https://www.gov.br/igualdaderacial/pt-br/assuntos/copy2_of_noticias")

import requests
from bs4 import BeautifulSoup
import gspread
from google.oauth2.service_account import Credentials
import pandas as pd
import urllib3
from datetime import datetime
import pytz

# Suprimir avisos de requisi√ß√£o insegura
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Mapeamento dos meses em ingl√™s
MESES_EN = {
    'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',
    'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'
}

def initialize_sheet():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = Credentials.from_service_account_file('credentials.json', scopes=scope)
    client = gspread.authorize(creds)
    return client.open_by_key('1G81BndSPpnViMDxRKQCth8PwK0xmAwH-w-T7FjgnwcY')

def get_already_scraped_urls(sheet):
    try:
        urls_sheet = sheet.worksheet("URLs")
    except gspread.exceptions.WorksheetNotFound:
        urls_sheet = sheet.add_worksheet(title="URLs", rows="1", cols="1")
        urls_sheet.append_row(["URLs"])
    return set(urls_sheet.col_values(1)[1:])

def add_scraped_url(sheet, url):
    urls_sheet = sheet.worksheet("URLs")
    urls_sheet.append_row([url])

def scrape_cfm_news(url, sheet):
    headers = {'User-Agent': 'Mozilla/5.0'}
    data_list = []

    try:
        response = requests.get(url, headers=headers, verify=False)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        already_scraped_urls = get_already_scraped_urls(sheet)

        # Captura do t√≠tulo
        title_tag = soup.find('h3')
        title = title_tag.text.strip() if title_tag else 'N/A'

        # Captura do link
        link_tag = soup.find('a', class_='c-default', href=True)
        link = link_tag['href'] if link_tag else 'N/A'

        if link in already_scraped_urls:
            print(f"üîÅ URL j√° raspada: {link}")
            return

        # Captura da data no formato: <h3>dia</h3> e <div>m√™s<br>ano</div>
        date_div = soup.find('div', class_='noticia-date')
        day_tag = date_div.find('h3') if date_div else None
        month_year_div = date_div.find('div') if date_div else None

        day = day_tag.text.strip().zfill(2) if day_tag else '01'

        if month_year_div:
            parts = list(month_year_div.stripped_strings)
            if len(parts) >= 2:
                month_abbr = parts[0][:3].capitalize()
                month = MESES_EN.get(month_abbr, '01')
                year = parts[1].strip()
            else:
                month, year = '01', '2025'
        else:
            month, year = '01', '2025'

        data_formatada = f"{day}/{month}/{year}"

        # Verifica√ß√£o com a data de hoje
        tz = pytz.timezone("America/Sao_Paulo")
        hoje = datetime.now(tz).strftime("%d/%m/%Y")
        if data_formatada != hoje:
            print(f"üìå Not√≠cia ignorada ‚Äì data diferente de hoje: {data_formatada}")
            return

        # Captura da descri√ß√£o
        description_tag = soup.find('p')
        description = description_tag.text.strip() if description_tag else 'N/A'

        data_list.append([data_formatada, title, description, link])

        worksheet = sheet.worksheet("cfm")
        worksheet.append_rows(data_list)
        add_scraped_url(sheet, link)

        print(f"‚úÖ Not√≠cia do dia inserida: {data_formatada} | {title}")

    except requests.exceptions.RequestException as err:
        print(f"‚ùå Erro na requisi√ß√£o: {err}")

# Executar
sheet = initialize_sheet()
url = "https://portal.cfm.org.br/noticias/?s="
scrape_cfm_news(url, sheet)

# Consed

import requests
from bs4 import BeautifulSoup
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from datetime import datetime
import pytz

def initialize_sheet(sheet_id, aba_nome):
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)
    client = gspread.authorize(creds)
    return client.open_by_key(sheet_id).worksheet(aba_nome)

def get_or_create_url_sheet(sheet):
    try:
        return sheet.spreadsheet.worksheet("URLs")
    except gspread.exceptions.WorksheetNotFound:
        ws = sheet.spreadsheet.add_worksheet(title="URLs", rows="1", cols="1")
        ws.append_row(["URLs"])
        return ws

def get_already_scraped_urls(sheet):
    url_sheet = get_or_create_url_sheet(sheet)
    return set(url_sheet.col_values(1)[1:])  # Ignora cabe√ßalho

def add_scraped_url(sheet, url):
    url_sheet = get_or_create_url_sheet(sheet)
    url_sheet.append_row([url])

def raspar_noticias_consed(sheet, data_desejada=None, max_pages=5):
    base_url = "https://www.consed.org.br/noticias?page="
    tz = pytz.timezone('America/Sao_Paulo')

    if not data_desejada:
        data_desejada = datetime.now(tz).strftime("%d/%m/%Y")

    urls_lidas = get_already_scraped_urls(sheet)
    novas_linhas = []
    page = 1

    while page <= max_pages:
        url = f"{base_url}{page}"
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Erro ao acessar a p√°gina {url}: {e}")
            break

        soup = BeautifulSoup(response.content, 'html.parser')
        noticias = soup.find_all('a', href=True)

        if not noticias:
            break

        for item in noticias:
            titulo_tag = item.find('h2')
            data_tag = item.find('small')
            descricao_tag = item.find('p')

            if not (titulo_tag and data_tag and descricao_tag):
                continue

            titulo = titulo_tag.text.strip()
            descricao = descricao_tag.text.strip()
            data_raw = data_tag.text.strip()
            link = item['href']
            full_link = link if link.startswith("http") else f"https://www.consed.org.br{link}"

            try:
                data_dt = datetime.strptime(data_raw, "%d/%m/%Y")
                data_fmt = data_dt.strftime("%d/%m/%Y")
            except ValueError:
                continue

            if data_fmt != data_desejada or full_link in urls_lidas:
                continue

            novas_linhas.append([data_dt, titulo, descricao, full_link])
            add_scraped_url(sheet, full_link)

        page += 1

    if novas_linhas:
        sheet.append_rows(novas_linhas, value_input_option='USER_ENTERED')
        print(f"‚úÖ {len(novas_linhas)} not√≠cia(s) adicionada(s) da Consed.")
    else:
        print("‚ÑπÔ∏è Nenhuma nova not√≠cia encontrada para a data informada.")

# --- Execu√ß√£o ---
if __name__ == "__main__":
    SHEET_ID = '1G81BndSPpnViMDxRKQCth8PwK0xmAwH-w-T7FjgnwcY'
    aba = 'consed'
    sheet = initialize_sheet(SHEET_ID, aba)
    raspar_noticias_consed(sheet)

# Undime

import re
import pytz
import gspread
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials

def initialize_sheet(sheet_id):
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)
    client = gspread.authorize(creds)
    return client.open_by_key(sheet_id)

def get_or_create_worksheet(sheet, name, headers=None):
    try:
        ws = sheet.worksheet(name)
    except gspread.exceptions.WorksheetNotFound:
        ws = sheet.add_worksheet(title=name, rows="100", cols="20")
        if headers:
            ws.append_row(headers)
    return ws

def get_already_scraped_urls(sheet):
    urls_sheet = get_or_create_worksheet(sheet, "URLs", headers=["URLs"])
    return set(urls_sheet.col_values(1)[1:])  # ignora o cabe√ßalho

def add_scraped_url(sheet, url):
    urls_sheet = get_or_create_worksheet(sheet, "URLs", headers=["URLs"])
    urls_sheet.append_row([url])

def raspar_noticias_undime(sheet, data_desejada=None):
    tz = pytz.timezone('America/Sao_Paulo')
    if data_desejada is None:
        data_desejada = datetime.now(tz).strftime('%d/%m/%Y')

    url = "https://undime.org.br/noticia/page/1"
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erro ao acessar o site: {e}")
        return

    soup = BeautifulSoup(response.content, 'html.parser')
    noticias = soup.find_all('div', class_='noticia mt-4 shadow2 p-3 border-radius')
    urls_ja_lidas = get_already_scraped_urls(sheet)
    undime_sheet = get_or_create_worksheet(sheet, "undime", headers=["Data", "T√≠tulo", "Descri√ß√£o", "URL"])

    novas_linhas = []

    for noticia in noticias:
        # LINK e data no link
        link_tag = noticia.find('a', href=True)
        if not link_tag:
            continue

        link = "https://undime.org.br" + link_tag['href']
        if link in urls_ja_lidas:
            continue

        data_match = re.search(r'\d{2}-\d{2}-\d{4}', link)
        if not data_match:
            continue

        try:
            data_dt = datetime.strptime(data_match.group(), '%d-%m-%Y')
            data_fmt = data_dt.strftime('%d/%m/%Y')
        except ValueError:
            continue

        if data_fmt != data_desejada:
            continue

        # T√≠tulo
        titulo_tag = noticia.find('h4')
        titulo = titulo_tag.text.strip() if titulo_tag else "T√≠tulo n√£o dispon√≠vel"

        # Descri√ß√£o
        try:
            descricao_tag = noticia.select_one('p.acessibilidade > a')
            descricao = descricao_tag.text.strip() if descricao_tag else "Descri√ß√£o n√£o dispon√≠vel"
        except:
            descricao = "Descri√ß√£o n√£o dispon√≠vel"

        novas_linhas.append([data_dt, titulo, descricao, link])
        add_scraped_url(sheet, link)

    if novas_linhas:
        undime_sheet.append_rows(novas_linhas, value_input_option='USER_ENTERED')
        print(f"‚úÖ {len(novas_linhas)} not√≠cia(s) adicionada(s) da Undime.")
    else:
        print("‚ÑπÔ∏è Nenhuma nova not√≠cia da Undime para hoje.")

# ------------------------------
# Execu√ß√£o
if __name__ == "__main__":
    SHEET_ID = '1G81BndSPpnViMDxRKQCth8PwK0xmAwH-w-T7FjgnwcY'
    sheet = initialize_sheet(SHEET_ID)
    raspar_noticias_undime(sheet)

# ANS

import requests
import gspread
import pytz
from bs4 import BeautifulSoup
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials

def initialize_sheet(sheet_id):
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)
    client = gspread.authorize(creds)
    return client.open_by_key(sheet_id)

def get_or_create_worksheet(sheet, name, headers=None):
    try:
        ws = sheet.worksheet(name)
    except gspread.exceptions.WorksheetNotFound:
        ws = sheet.add_worksheet(title=name, rows="100", cols="20")
        if headers:
            ws.append_row(headers)
    return ws

def get_already_scraped_urls(sheet):
    urls_sheet = get_or_create_worksheet(sheet, "URLs", headers=["URLs"])
    return set(urls_sheet.col_values(1)[1:])  # Ignora cabe√ßalho

def add_scraped_url(sheet, url):
    urls_sheet = get_or_create_worksheet(sheet, "URLs", headers=["URLs"])
    urls_sheet.append_row([url])

def raspar_noticias_ans(sheet, data_desejada=None):
    tz = pytz.timezone('America/Sao_Paulo')
    if data_desejada is None:
        data_desejada = datetime.now(tz).strftime("%d/%m/%Y")

    url = "https://www.gov.br/ans/pt-br/assuntos/noticias"
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erro ao acessar {url}: {e}")
        return

    soup = BeautifulSoup(response.content, 'html.parser')
    noticias = soup.find_all('div', class_='conteudo')
    urls_ja_lidas = get_already_scraped_urls(sheet)
    ans_sheet = get_or_create_worksheet(sheet, "ans", headers=["Data", "Minist√©rio", "Subt√≠tulo", "T√≠tulo", "Link"])

    novas_linhas = []

    for noticia in noticias:
        try:
            subtitulo_tag = noticia.find('div', class_='subtitulo-noticia')
            subtitulo = subtitulo_tag.text.strip() if subtitulo_tag else "Subt√≠tulo n√£o dispon√≠vel"

            titulo_tag = noticia.find('a', href=True)
            titulo = titulo_tag.text.strip() if titulo_tag else "T√≠tulo n√£o dispon√≠vel"
            link = titulo_tag['href'] if titulo_tag else "Link n√£o dispon√≠vel"

            data_tag = noticia.find('span', class_='data')
            data_raw = data_tag.text.strip() if data_tag else None

            if not data_raw or not link or link in urls_ja_lidas:
                continue

            try:
                data_dt = datetime.strptime(data_raw, '%d/%m/%Y')
            except ValueError:
                continue

            data_fmt = data_dt.strftime('%d/%m/%Y')
            if data_fmt != data_desejada:
                continue

            linha = [data_dt, "ANS", subtitulo, titulo, link]
            novas_linhas.append(linha)
            add_scraped_url(sheet, link)

        except Exception as e:
            print(f"Erro ao processar not√≠cia: {e}")
            continue

    if novas_linhas:
        ans_sheet.append_rows(novas_linhas, value_input_option='USER_ENTERED')
        print(f"‚úÖ {len(novas_linhas)} not√≠cia(s) da ANS adicionada(s).")
    else:
        print("‚ÑπÔ∏è Nenhuma nova not√≠cia da ANS para hoje.")

# ------------------------------
# Execu√ß√£o
if __name__ == "__main__":
    SHEET_ID = '1G81BndSPpnViMDxRKQCth8PwK0xmAwH-w-T7FjgnwcY'
    sheet = initialize_sheet(SHEET_ID)
    raspar_noticias_ans(sheet)

# ANVISA

import requests
import gspread
import pytz
import urllib3
from bs4 import BeautifulSoup
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials

# Suprimir avisos SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def initialize_sheet():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)
    client = gspread.authorize(creds)
    return client.open_by_key('1G81BndSPpnViMDxRKQCth8PwK0xmAwH-w-T7FjgnwcY')

def get_or_create_worksheet(sheet, name, headers=None):
    try:
        return sheet.worksheet(name)
    except gspread.exceptions.WorksheetNotFound:
        ws = sheet.add_worksheet(title=name, rows="100", cols="20")
        if headers:
            ws.append_row(headers)
        return ws

def get_already_scraped_urls(sheet):
    urls_sheet = get_or_create_worksheet(sheet, "URLs", headers=["URLs"])
    return set(urls_sheet.col_values(1)[1:])  # Ignora cabe√ßalho

def add_scraped_url(sheet, url):
    urls_sheet = get_or_create_worksheet(sheet, "URLs", headers=["URLs"])
    urls_sheet.append_row([url])

def raspar_anvisa_do_dia(sheet):
    tz = pytz.timezone('America/Sao_Paulo')
    data_hoje = datetime.now(tz)
    data_hoje_str = data_hoje.strftime("%d/%m/%Y")
    url = 'https://www.gov.br/an

# FIOCRUZ

import requests
import gspread
import pytz
import urllib3
from bs4 import BeautifulSoup
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials

# Suprimir avisos SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def initialize_sheet():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)
    client = gspread.authorize(creds)
    return client.open_by_key('1G81BndSPpnViMDxRKQCth8PwK0xmAwH-w-T7FjgnwcY')

def get_or_create_worksheet(sheet, name, headers=None):
    try:
        return sheet.worksheet(name)
    except gspread.exceptions.WorksheetNotFound:
        ws = sheet.add_worksheet(title=name, rows="100", cols="20")
        if headers:
            ws.append_row(headers)
        return ws

def get_already_scraped_urls(sheet):
    urls_sheet = get_or_create_worksheet(sheet, "URLs", headers=["URLs"])
    return set(urls_sheet.col_values(1)[1:])

def add_scraped_url(sheet, url):
    urls_sheet = get_or_create_worksheet(sheet, "URLs", headers=["URLs"])
    urls_sheet.append_row([url])

def raspar_fiocruz(sheet):
    tz = pytz.timezone('America/Sao_Paulo')
    data_desejada = datetime.now(tz)
    data_desejada_str = data_desejada.strftime("%d/%m/%Y")
    url = "https://fiocruz.br/noticias"

    try:
        response = requests.get(url, verify=False, timeout=10)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erro ao acessar Fiocruz: {e}")
        return

    soup = BeautifulSoup(response.text, "html.parser")
    blocos = soup.select("div.views-row")
    urls_existentes = get_already_scraped_urls(sheet)
    aba_fiocruz = get_or_create_worksheet(sheet, "fiocruz", headers=["Data", "T√≠tulo", "Descri√ß√£o", "URL"])

    novas_linhas = []

    for bloco in blocos:
        try:
            data_tag = bloco.find("div", class_="data-busca")
            data_text = data_tag.find("time").text.strip() if data_tag else None

            if not data_text or data_text != data_desejada_str:
                continue

            titulo_tag = bloco.find("div", class_="titulo-busca").find("a")
            titulo = titulo_tag.text.strip()
            link = "https://www.fiocruz.br" + titulo_tag["href"]

            if link in urls_existentes:
                continue

            chamada_tag = bloco.find("div", class_="chamada-busca")
            descricao = chamada_tag.text.strip() if chamada_tag else "Descri√ß√£o n√£o dispon√≠vel"

            data_dt = datetime.strptime(data_text, "%d/%m/%Y")
            linha = [data_dt, titulo, descricao, link]

            novas_linhas.append(linha)
            add_scraped_url(sheet, link)

        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao processar not√≠cia da Fiocruz: {e}")
            continue

    if novas_linhas:
        aba_fiocruz.append_rows(novas_linhas, value_input_option='USER_ENTERED')
        print(f"‚úÖ {len(novas_linhas)} not√≠cia(s) da Fiocruz adicionadas.")
    else:
        print("‚ÑπÔ∏è Nenhuma nova not√≠cia da Fiocruz para hoje.")

# Execu√ß√£o
if __name__ == "__main__":
    sheet = initialize_sheet()
    raspar_fiocruz(sheet)

import requests
from bs4 import BeautifulSoup
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
import pytz
import urllib3

# Suprimir avisos de requisi√ß√£o insegura
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Mapeamento dos meses em portugu√™s (siglas)
MESES = {
    'jan': '01',
    'fev': '02',
    'mar': '03',
    'abr': '04',
    'mai': '05',
    'jun': '06',
    'jul': '07',
    'ago': '08',
    'set': '09',
    'out': '10',
    'nov': '11',
    'dez': '12'
}

def initialize_sheet():
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = Credentials.from_service_account_file('credentials.json', scopes=scope)
    client = gspread.authorize(creds)
    return client.open_by_key('1G81BndSPpnViMDxRKQCth8PwK0xmAwH-w-T7FjgnwcY')

def get_already_scraped_urls(sheet):
    try:
        urls_sheet = sheet.worksheet("URLs")
    except gspread.exceptions.WorksheetNotFound:
        urls_sheet = sheet.add_worksheet(title="URLs", rows="1", cols="1")
        urls_sheet.append_row(["URLs"])
    return set(urls_sheet.col_values(1)[1:])

def add_scraped_url(sheet, url):
    urls_sheet = sheet.worksheet("URLs")
    urls_sheet.append_row([url])

def scrape_cfm_news(url, sheet):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    try:
        response = requests.get(url, headers=headers, verify=False, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        # J√° lidas
        already_scraped_urls = get_already_scraped_urls(sheet)

        # Dados
        title_tag = soup.find('h3')
        title = title_tag.text.strip() if title_tag else 'N/A'

        link_tag = soup.find('a', class_='c-default', href=True)
        link = link_tag['href'] if link_tag else 'N/A'

        if link in already_scraped_urls:
            print(f"URL j√° raspada: {link}")
            return

        # Data
        date_div = soup.find('div', class_='noticia-date')
        day_tag = date_div.find('h3') if date_div else None
        month_year_div = date_div.find('div') if date_div else None

        day = day_tag.text.strip() if day_tag else ''
        month, year = '', ''
        if month_year_div:
            parts = month_year_div.get_text(separator=" ").split()
            if len(parts) >= 2:
                month = MESES.get(parts[0].lower()[:3], '01')  # usa sigla com fallback
                year = parts[1]

        # Montar datetime real
        try:
            data_dt = datetime.strptime(f"{day}/{month}/{year}", "%d/%m/%Y")
        except Exception as e:
            print(f"Erro ao converter data: {e}")
            return

        # Filtrar apenas not√≠cias do dia atual (Brasil)
        hoje_br = datetime.now(pytz.timezone('America/Sao_Paulo')).date()
        if data_dt.date() != hoje_br:
            print(f"‚ÑπÔ∏è Not√≠cia n√£o √© de hoje ({data_dt.strftime('%d/%m/%Y')})")
            return

        # Descri√ß√£o
        description_tag = soup.find('p')
        description = description_tag.text.strip() if description_tag else 'N/A'

        # Planilha
        try:
            worksheet = sheet.worksheet("cfm")
        except gspread.exceptions.WorksheetNotFound:
            worksheet = sheet.add_worksheet(title="cfm", rows="100", cols="4")
            worksheet.append_row(["Data", "T√≠tulo", "Descri√ß√£o", "Link"])

        # Inserir com tipo DATA real
        worksheet.append_row([data_dt, title, description, link], value_input_option='USER_ENTERED')

        add_scraped_url(sheet, link)
        print(f"‚úÖ Not√≠cia adicionada: {title}")

    except Exception as e:
        print(f"‚ùå Erro geral: {e}")

# Executar
if __name__ == "__main__":
    sheet = initialize_sheet()
    scrape_cfm_news("https://portal.cfm.org.br/noticias/?s=", sheet)
